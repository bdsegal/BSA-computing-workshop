---
title: "Reshaping and manipulating rectangular datasets"
author: "Brian Segal"
date: "Feb 12, 2016"
number_sections: yes
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```


# Split, apply, combine, and plot: Enter Mr. Wickham

Split, apply, combine is a general approach to aggregating and summarizing data that Hadley Wickham popularized in the R community. For background, see Wickham's 2011 paper, [The Split-Apply-Combine Strategy for Data Analysis](http://www.jstatsoft.org/article/view/v040i01). His current implementation is the `dplyr` package, which replaced the `plyr` package, and is much faster.

We can also use the `reshape2` package to switch between long and wide formats. While `reshape2` can be used for a variety of purposes, preparing data for `ggplot2` is probably one of the most common.

There are several good tutorials for these packages (see Hadley Wickham's [Introduction to `dplyr`](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html), Sean Anderson's [Introduction to `reshape2`](http://seananderson.ca/2013/10/19/reshape.html), and Hadley Wickham's [website for the `reshape`](http://had.co.nz/reshape/). Wickham's [website for `ggplot2`](http://docs.ggplot2.org/current/) describes all of the options and gives many examples. Since there are so many existing resources for `dplyr`, `reshape2`, and `ggplot2`, I thought it would be most helpful to give a very brief overview of each, and then give some examples where we use them together.

After experimenting with `ggplot`, we'll see how we can use `ggvis` to make similar plots that are also interactive.

# Overviews

## `dplyr`

## `reshape2`

## `ggplot2`

# Examples

Let's take a look at the electricity consumption data in killowat-hours (KWH). We'll also use the the sampling weights, `NWEIGHT`, and [U.S. Census regions](http://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf) `REGIONC`. There are a few parsing errors, which I deal with in (link to tutorial). For now, we can ignore the errors.

```{r}
library(readr)
data <- read_csv("http://www.eia.gov/consumption/residential/data/2009/csv/recs2009_public.csv")
```

Before jumping into the [Hadleyverse](http://blog.revolutionanalytics.com/2015/03/hadleyverse.html), let's use R base functions to plot regional means.

```{r}
data$REGIONC <- factor(data$REGIONC, labels=c("Northeast","Midwest","South","West"))

kwhMean <- tapply(data$KWH, data$REGIONC, mean)
barplot(kwhMean, ylab="kWh", xlab="Region")
```

Now let's use Wickham's packages.

```{r}
library(reshape2)
library(dplyr)
library(ggplot2)

# Use dplyr to get both the weighted and unweighted means
regionKwhMean <- data %>%
  group_by(REGIONC) %>%
  summarize(
    weighted = weighted.mean(x = KWH, w = NWEIGHT),
    unweighted = mean(KWH)
  )

# Use reshape2 to prep for gpplot2
regionKwhMeanM <- melt(regionKwhMean, id.vars = "REGIONC")

# plot with ggplot
ggplot(aes(x = factor(REGIONC), y = value, fill = variable), data = regionKwhMeanM)+
  geom_bar(stat = "identity", position = "dodge")+
  theme_bw(16)+
  labs(x="Region", y="kWh", title = "Regional Kilowatt-Hour Usage 2009 \n Weighted vs Unweighted Mean")+
  scale_fill_discrete("")

```

It looks like the South uses a lot of electricity. Is it because they have a lot of [cooling degree days](http://www.srh.noaa.gov/key/?n=climate_heat_cool) `CDD65`?

>Note: I use a 1/4 scaling factor to induce normality for `KWH`, which makes the standard errors more reliable for the smoothing estimates. We'll talk more about this and `geom_smooth` below.

```{r}
ggplot(aes(x = CDD65, y = KWH^(1/4)), data=data)+
  geom_point(alpha=0.03)+
  geom_smooth()+
  facet_wrap(~REGIONC)+
  theme_bw(18)+
  geom_hline(yintercept=mean(data$KWH^(1/4)), color="red", linetype="dashed")+
  labs(y=expression("kWh"^"1/4"), x="Cooling days (base temp 65 F)")
```

For a more direct visual comparison, instead of faceting by region, let's color by regions.

```{r}
ggplot(aes(x = CDD65, y = KWH^(1/4), color=REGIONC), data=data)+
  geom_point(alpha=0.075)+
  geom_smooth()+
  theme_bw(18)+
  labs(y=expression("kWh"^"1/4"), x="Cooling days (base temp 65 F)")+
  scale_color_discrete("Region")
```

Even after adjusting for heating days, the south seems to use more electricity.

Now for that 1/4 scaling factor. First let's look at a histogram of `KWH`.

```{r}
qplot(x=data$KWH, geom="histogram", xlab="kWh")+theme_classic(18)
```

That's definitely not a normal distribuiton. Maybe it's an exponential? What happens if we take a log transform?
```{r}
qplot(x=log(data$KWH), geom="histogram", xlab="log(kWh)")+theme_classic(18)
```

That's still skewed. Raising `KWH` to a 1/4 seems to induce normality.
```{r}
qplot(x=data$KWH^(1/4), geom="histogram", xlab=expression("kWh"^"1/4"))+theme_classic(18)
```

Let's spiff up the plot by using a kernel density estimator and overlaying a normal distribution in red. `geom_density` calls the `density` function from the `stats` package.
```{r}
ggplot(aes(x=KWH^(1/4)), data=data)+
  geom_density()+
  stat_function(fun=dnorm, arg = list(mean=mean(data$KWH^(1/4)), sd=sd(data$KWH^(1/4))), 
                color="red")+
  theme_classic(18)+
  labs(x=expression("kWh"^"1/4"), y="Density")
```

We could also make the lines thicker and fill in the kernel density estimator.
```{r}
ggplot(aes(x=KWH^(1/4)), data=data)+
  geom_density(color="grey", fill="grey", size=1)+
  stat_function(fun=dnorm, arg = list(mean=mean(data$KWH^(1/4)), sd=sd(data$KWH^(1/4))), 
                color="red", size=1)+
  theme_classic(18)+
  labs(x=expression("kWh"^"1/4"), y="Density")
```

We can also look at a q-q plot.
```{r}
ggplot(aes(sample=KWH^(1/4)), data=data)+
  stat_qq()+
  theme_classic(18)
```

Scaling the outcome is convenient for mathematical reasons, but it makes interpretation difficult. The gamma distribution is a flexible choice for this type of data, and it seems to describe the distribution of `KWH` well.

```{r}
# estimate parameters of gamma with method of moments
mu <- mean(data$KWH)
sigma2 <- var(data$KWH)
alpha <- mu^2/sigma2
beta <- sigma2/mu
	
ggplot(aes(x=KWH), data=data)+
  geom_density(color="grey", fill="grey", size=1)+
  stat_function(fun=dgamma, arg = list(shape=alpha, scale=beta), 
                color="red", size=1)+
  theme_classic(18)+
  labs(x="kWh", y="Density")
```

Then we can repeat make the same plots as above, but on the original scale, and ask the `gam` function to use the Gamma distribution to fit the smooth.
```{r, warning=TRUE, message=TRUE}
ggplot(aes(x = CDD65, y = KWH, color=REGIONC), data=data)+
  geom_point(alpha=0.075)+
  geom_smooth(method="gam", method.args=list(family=Gamma))+
  theme_bw(18)+
  labs(y="kWh", x="Cooling days (base temp 65 F)")+
  scale_color_discrete("Region")+
  scale_y_continuous(lim=c(0,50000))
```

The `gam` algorithm didn't converge, so these smooths might not be quite right, but they seem reasonable for exploratory purposes. We also didn't use the sampling weights in the smooth.

For more than 1,000 observations, `geom_smooth` calls the `gam` function from SImon Wood's [`mgcv`](https://cran.r-project.org/web/packages/mgcv/mgcv.pdf) package. `gam` stands for generalized additive model, but in this case, the smooth is a function of only one variable -- cooling days. `gam` uses cubic regression splines by default, so the smooths in the plots above are cubic regression splines within each region. As an aside, I highly recommend Simon Wood's textbook, [Generalized Additive Models: An Introduction with R](https://www.crcpress.com/Generalized-Additive-Models-An-Introduction-with-R/Wood/9781584884743).

# Exercises

1. Make the same plots as above, but with `DIVISION` in place of `REGIONC`.
2. Facet the plots from exercise 1 by `REGIONC`.

# `ggvis`

`ggvis`, also by Hadley Wickham, is more recent package. It uses the same piping operators `%>%` as `dplyr`, and tends to render more quickly than `ggplot2`. This makes it a good complement to `Shiny`, which Mathieu will cover in a future workshop. You can also make interactive plots in `ggvis` without Siny.

Let's remake some of the above plots in ggvis.

# Exercises

Repeat with DIVISION -- color by DIVISION and facet by region. Any other variables explain the difference?

```{r, eval=FALSE}
ggplot(aes(x = YEARMADE, y = TOTALBTU^(1/3)), data=data)+
  geom_point(alpha=0.05)+
  geom_smooth()+
  facet_wrap(~REGIONC)+
  theme_bw()

ggplot(aes(x=TOTALBTU), data=data)+geom_histogram()
ggplot(aes(x=KWH^(1/3)), data=data)+geom_histogram()

summary(data$CDD65)

regionBtuMean <- data %>%
  group_by(REGIONC) %>%
  summarize(
    weighted = weighted.mean(x = TOTALBTU, w = NWEIGHT),
    unweighted = mean(TOTALBTU)
  )



# melting to prep for gpplot2
melted <- melt(regionBtuMean, id.vars = "REGIONC")

# tips: change font size inside theme
ggplot(aes(x = factor(REGIONC), y = value, fill = variable),
  data = melted)+
  geom_bar(stat = "identity", position = "dodge")+
  theme_bw(16)+
  labs(x="region", y="BTU", title = "Total BTU Usage 2009 \n Weighted vs Unweighted Mean")+
  scale_fill_discrete("")

ggplot(aes(x = CDD65, y = TOTALBTU^(1/3)), data=data)+
  geom_point(alpha=0.05)+
  geom_smooth()+
  facet_wrap(~REGIONC)+
  theme_bw()
```

# TODO
  1. Make more plots, some with multiple layers, smooths, etc.
  2. More examples of dplyr
  3. Do regressions within each groupby level
  4. use dcast and acast to recast the molten data
  5. Introduce ggvis, and explain that it is faster -- better for Shiny
      also interactive
  6. Discuss data.table? -- maybe give example with bootstrap, and
     give link to data.table tutorial
  7. Introduce rgl and spatial/geographic plots
  8. Make exercises
  9. push to github, add .aux and other latex files to .gitingore, and
     .Rhistory -- make this global
  10. Does it make sense to weight within region?
  11. Remove some code files?
  12. Make sure file names and headings make sense
  13. Use slides as outline for talk?
  14. Be more concise!

Note: More recently, Hadley Wickham came out with the `tidyr` package. In [introducing `tidyr`](http://blog.rstudio.org/2014/07/22/introducing-tidyr) Wickham says:

>Just as reshape2 did less than reshape, tidyr does less than reshape2. It's designed specifically for tidying data, not general reshaping. In particular, existing methods only work for data frames, and tidyr never aggregates.

