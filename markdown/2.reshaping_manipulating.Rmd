---
title: "Reshaping and manipulating rectangular datasets"
author: "Brian Segal"
date: "Feb 12, 2016"
number_sections: yes
---

# Load and inspect data

Let's load the data from the [2009 residential energy consumption survey (RECS)](http://www.eia.gov/consumption/residential/data/2009/index.cfm?view=microdata). As described on the website, the 2009 RECS included 12,083 households selected at random using a complex multistage, area-probability sample design.

```{r, warning=FALSE, message=FALSE}
library(readr)

parent <- dirname(getwd())

data <- read_csv(file.path(parent, 'data', 'recs2009.csv'),
  progress=FALSE)
prob <- problems(data)
badCol <- unique(prob$col)
badCol
```

There were problems reading in eight columns. There were only `r nrow(prob)` observations with problems, but before proceeding, we should figure out if these problems indicate larger data quality issues, especially if we are interested in the problematic columns. To start, we can consult the documentation, and if we deem it sufficiently important, we could get in touch with the staff at EIA.

In this case, the Energy Information Administration (EIA) provides a layout file. We can use it to get more information about the problematic columns.

```{r}
# get the layout file
layout <- read.csv("http://www.eia.gov/consumption/residential/data/2009/csv/public_layout.csv")
str(layout)

badColInfo <- layout[which(layout$Variable.Name %in% badCol),]
badColInfo
```

I've never worked with this data before, but '`r as.character(badColInfo[1,2])`' sounds like it should be restricted to be a non-negative integer. I suspect that most of the data are ok and that there are just few problematic values, but I would look into it further before doing a serious analysis.

Assuming that we determined that the data were ok, let's use the layout file to tell `readr` how to read the data properly. See `vignette("column-types")` for more information about column types (enter the text into an R session after loading the `readr` package).

```{r}
colTypes <- as.character(layout$Variable.Type)
colTypes <- gsub("Numeric","d", colTypes) # d for double
colTypes <- gsub("Character","c", colTypes) # c for character
colTypes <- paste(colTypes, collapse="") # concatenate into 1 string

data <- read_csv(file.path(parent, 'data', 'recs2009.csv'),
  col_types = colTypes,
  progress = FALSE)
```

This is one way to solve the problem, though now every column is a `double`, so the data take up more space. This isn't a big dataset, so it's not a problem, but in other cases you might want to specify all the numeric columns as `int`, accept for the eight problematic columns.

We should also take a look at the missing data patterns before moving on. We'll use the [`pheatmap`](https://cran.r-project.org/web/packages/pheatmap/index.html) package to visualize the first few rows of missing data. The [`mi`](https://cran.r-project.org/web/packages/mi/index.html) package is also useful for inspecting missing data patterns, though it seems to work best for smaller datasets.

```{r, eval=FALSE}
library(pheatmap)

dataNA <- is.na(data)
mode(dataNA) <- "integer"
pheatmap(dataNA[1:500,], cluster_rows = FALSE, cluster_cols = FALSE)
```

It looks like there's a variable all the way on the left that is missing frequently, as well as a few on the right. Let's see what they are.

```{r, eval=FALSE}
colsWithNA <- which(apply(is.na(data), 2, sum) > 0)
colsWithNA
barplot(colsWithNA)
```

We should keep these variables in mind when we analyze the data, because the missingess might introduce bias.

For the rest of the workshop, let's take a look at the electricity consumption data in Killowat-hours (KWH). We'll keep those columns, as well as the household id variable `DOEID` and the sampling weights `NWEIGHT`.

```{r, eval=FALSE}
kwhCols <- grep("KWH", colnames(data))
idWeightCols <- which(colnames(data) %in% c("DOEID","NWEIGHT"))

kwhData <- data[,c(idWeightCols,kwhCols)]
str(kwhData)

layout[kwhCols,1:2]
```

# Split, apply, combine ... and plot

Split, apply, combine is a general approach to aggregating and summarizing data that Hadley Wickham popularized in the R community. For background, see Wickham's 2011 paper, [The Split-Apply-Combine Strategy for Data
Analysis](http://www.jstatsoft.org/article/view/v040i01). His current implementation is the `dplyr` package, which replaced the `plyr` package, and is much faster.

We can also use the `reshape2` package to switch between long and wide formats. While `reshape2` can be used for a variety of purposes, preparing data for `ggplot2` is probably one of the most common.

There are already several good tutorials for these packages (see Hadley Wickham's [Introduction to `dplyr`](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html), Sean Anderson's [Introduction to `reshape2`](http://seananderson.ca/2013/10/19/reshape.html), and Hadley Wickham's [website for the `reshape`](http://had.co.nz/reshape/). Wickham's [website for `ggplot2`](http://docs.ggplot2.org/current/) describes all of the options and gives many examples. Since there are so many existing resources on these packages, I thought it would be most helpful to give some examples where we use `dplyr`, `reshape2`, and `ggplot2` together.

```{r}
library(reshape2)
library(dplyr)
library(ggplot2)

# base R approach for getting the means
kwhMean <- tapply(data$KWH, data$REGIONC, mean)
barplot(kwhMean, ylab="kwh", xlab="region")

# Now using dplyr to get both the weighted and unweighted
# %>% are pipes
regionKwhMean <- data %>%
  group_by(REGIONC) %>%
  summarize(
    weighted = weighted.mean(x = KWH, w = NWEIGHT),
    unweighted = mean(KWH)
  )

# melting to prep for gpplot2
regionKwhMeanM <- melt(regionKwhMean, id.vars = "REGIONC")

# tips: change font size inside theme
ggplot(aes(x = factor(REGIONC), y = value, fill = variable),
  data = regionKwhMeanM)+
  geom_bar(stat = "identity", position = "dodge")+
  theme_bw(14)+
  labs(x="region", y="kwh", title = "Regional Kilowatt-Hour Usage 2009 \n Weighted vs Unweighted Mean")+
  scale_fill_discrete("")

```

# TODO
  1. Make more plots, some with multiple layers, smooths, etc.
  2. More examples of dplyr
  3. Do regressions within each groupby level
  4. use dcast and acast to recast the molten data
  5. Introduce ggvis, and explain that it is faster -- better for Shiny
      also interactive
  6. Discuss data.table? -- maybe give example with bootstrap, and
     give link to data.table tutorial
  7. Introduce rgl and spatial/geographic plots
  8. Make exercises
  9. Put on github
  10. Does it make sense to weight within region?

Note: More recently, Hadley Wikham came out with the `tidyr` package. According to Wikham, on [introducing `tidyr`](http://blog.rstudio.org/2014/07/22/introducing-tidyr):

>Just as reshape2 did less than reshape, tidyr does less than reshape2. It's designed specifically for tidying data, not general reshaping. In particular, existing methods only work for data frames, and tidyr never aggregates.

