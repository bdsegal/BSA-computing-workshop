---
title: "Faster operations with `readr`, `dplyr` and `data.table`"
author: "Brian Segal"
date: "Feb 12, 2016"
number_sections: yes
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```
# Introduction

# Overview

## `dplyr`

To do: Introduce `dplyr` syntax and aims (focus is on syntax and organizing the analyst's tasks)

## `data.table`

To do: Introduce `data.table` syntax and aims (focus is on speed, memory, and concise syntax).

# Example

We'll bootstrap the mean kilowat-hours in the RECS data. For comparisons on larger datasets, please see Matt Dowle's [benchmark](https://github.com/Rdatatable/data.table/wiki/Benchmarks-%3A-Grouping) (this also compares to the pandas library for python).

```{r}
parent <- dirname(getwd())
B <- 10000
```

## Base R

```{r, cache = TRUE}

data <- read.csv(file.path(parent, 'data', 'recs2009.csv'))
n <- nrow(data)

bootBase <- array(NA, dim=c(B,4))

t_base <- system.time(for (b in 1:B){
  temp <- data[sample(n, replace=TRUE), c("KWH","REGIONC")]
  bootBase[b,] <- tapply(temp$KWH, temp$REGIONC, mean)
})[3]
```

## `dplyr`

Note: I don't think this is the best way to bootstrap with `dplyr`. See the [`broom` vignette](https://cran.r-project.org/web/packages/broom/vignettes/bootstrapping.html) for more information.

```{r, cache = TRUE}
library(readr)
library(dplyr)

data <- read_csv(file.path(parent, 'data', 'recs2009.csv'))

bootDplyr <- array(NA, dim=c(B,4))

t_dplyr <- system.time(for (b in 1:B){
  temp <- data %>% select(KWH, REGIONC) %>%
    sample_n(n, replace=TRUE) %>%
    group_by(REGIONC) %>%
    summarise(mean(KWH))
  bootDplyr[b,] <- unlist(temp[,2])
})[3]
```

## `data.table`

```{r, cache = TRUE}
library(data.table)

data <- fread(file.path(parent, 'data', 'recs2009.csv'))

bootData <- array(NA, dim=c(B,4))

t_data <- system.time(for (b in 1:B){
  bootData[b,] <- data[sample(n, replace=TRUE), mean(KWH), by=REGIONC][order(REGIONC),V1]
})[3]
```
## Comparison 

**Run time**

```{r}
compare <- c(t_base, t_dplyr, t_data)
names(compare) <- c("base", "dplyr", "data.table")
barplot(compare, ylab = "time (sec)")
```

**Bootstrap distributions**

```{r, cache = TRUE}
library(reshape2)
library(ggplot2)

# put all bootstrap results in a list for easier processing
bootResults <- list(bootBase, bootDplyr, bootData)
names(bootResults) <- c("base", "dplyr", "data.table")
for (i in 1:length(bootResults)){
  colnames(bootResults[[i]]) <- c("Northeast","Midwest","South","West")
}

# melt to get ready for ggplot2
bootMelt <- melt(bootResults)[,-1]
colnames(bootMelt) <- c("region", "mean", "method")

qplot(x=mean, data=as.data.frame(bootMelt), geom="histogram")+
  facet_grid(method ~ region, scale = "free")+
  theme_bw(16)+
  scale_x_continuous(breaks = seq(6,15,.5)*1000)
```

Note: For a more comprehensive comparison, you could repeat the above many times to get a distribution of run times.

# Exercises

1. With `dplyr` and `data.table`, bootstrap both the mean and the median, grouped by region. Calculate both quantities with a single call to `summary` and `data.table[]`, respectively. Use `B=100` iterations while you are testing your code.

2. With `dplyr` and `data.table` bootstrap the mean for all numeric variables, grouped by region. Do this with a single call to `dplyr` and `data.table`. The examples in this [stack overflow discussion](http://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly) have some useful tips, as well as commentary about `dplyr` vs `data.table`.

3. Watch [Grace Hopper explain nanoseconds](https://www.youtube.com/watch?v=JEpsKnWZrJ8)